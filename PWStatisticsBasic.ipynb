{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0eUjNn3AHvOdZDQsjS2jM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explain the different types of data (qualitative and quantitative) and provide examples of each. Discuss nominal, ordinal, interval, and ratio scales."
      ],
      "metadata": {
        "id": "k7fQTG-iBVLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibjQ4zDwBSUI"
      },
      "outputs": [],
      "source": [
        "# Data can be broadly classified into two categories: qualitative (categorical) and quantitative (numerical).\n",
        "\n",
        "# Qualitative Data: Represents characteristics or qualities and can't be measured numerically.\n",
        "\n",
        "# Examples:\n",
        "#   - Eye color (blue, brown, green)\n",
        "#   - Gender (male, female, other)\n",
        "#   - Blood type (A, B, AB, O)\n",
        "#   - Types of fruit (apple, banana, orange)\n",
        "#   - Country of origin\n",
        "\n",
        "\n",
        "# Quantitative Data: Represents measurable quantities and can be expressed numerically.Further divided into discrete and continuous.\n",
        "\n",
        "# Examples:\n",
        "#    - Height (170 cm, 185 cm)\n",
        "#    - Weight (65 kg, 72 kg)\n",
        "#    - Age (25 years, 30 years)\n",
        "#    - Number of students in a class\n",
        "#    - Income\n",
        "\n",
        "\n",
        "# Scales of Measurement:\n",
        "\n",
        "# 1. Nominal Scale:  Categorizes data into mutually exclusive groups with no inherent order.\n",
        "#    - Example: Colors of cars (red, blue, green), Types of animals.\n",
        "#    - Operations: Equality/Inequality only\n",
        "\n",
        "# 2. Ordinal Scale:  Data is categorized and ranked in a specific order, but the differences between categories are not uniform.\n",
        "#    - Example: Educational levels (high school, bachelor's, master's, doctorate), Customer satisfaction ratings (very satisfied, satisfied, neutral, dissatisfied, very dissatisfied).\n",
        "#    - Operations: Equality/Inequality, greater than/less than\n",
        "\n",
        "\n",
        "# 3. Interval Scale:  Data is ordered, and the differences between values are meaningful and consistent. However, there's no true zero point.\n",
        "#    - Example: Temperature in Celsius or Fahrenheit, Years (0 AD does not mean absence of time).\n",
        "#    - Operations: All the operations on ordinal scale + addition/subtraction\n",
        "\n",
        "# 4. Ratio Scale:  Data is ordered, the differences are meaningful and consistent, and there is a true zero point. This means zero represents the absence of the attribute being measured.\n",
        "#    - Example: Height, weight, age, income, number of items.\n",
        "#    - Operations: All previous operations + multiplication/division\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are the measures of central tendency, and when should you use each? Discuss the mean, median, **and** mode with examples and situations where each is appropriate."
      ],
      "metadata": {
        "id": "3QLe-HiKCNr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measures of Central Tendency: Mean, Median, and Mode\n",
        "\n",
        "# Mean: The average of a dataset.  It's calculated by summing all values and dividing by the number of values.\n",
        "\n",
        "# When to use:  Appropriate for data that is normally distributed (bell-shaped curve) and without significant outliers. Outliers heavily influence the mean.\n",
        "\n",
        "# Example:  Calculating the average test score of a class.  If the scores are generally clustered around the average, the mean provides a good representation of the typical score.\n",
        "\n",
        "\n",
        "# Example calculation:\n",
        "def calculate_mean(data):\n",
        "  if not data:  #Handle empty list\n",
        "    return 0\n",
        "  return sum(data) / len(data)\n",
        "\n",
        "\n",
        "data = [10, 20, 30, 40, 50]\n",
        "mean = calculate_mean(data)\n",
        "print(f\"Mean of data is {mean}\")\n",
        "\n",
        "# Median: The middle value when the dataset is ordered. If there's an even number of values, the median is the average of the two middle values.\n",
        "\n",
        "# When to use:  More robust to outliers than the mean.  Useful when the data is skewed (not symmetric) or contains extreme values.\n",
        "\n",
        "# Example:  Analyzing house prices in a neighborhood.  A few extremely expensive houses could skew the mean, but the median would represent a more typical price.\n",
        "\n",
        "# Example Calculation\n",
        "def calculate_median(data):\n",
        "  data.sort()\n",
        "  n = len(data)\n",
        "  if n % 2 == 0: # even number of elements\n",
        "    mid1 = data[n // 2 - 1]\n",
        "    mid2 = data[n // 2]\n",
        "    median = (mid1 + mid2) / 2\n",
        "  else:\n",
        "    median = data[n // 2]\n",
        "  return median\n",
        "\n",
        "data = [10, 20, 30, 40, 50, 1000]\n",
        "median = calculate_median(data)\n",
        "print(f\"Median of the data is {median}\")\n",
        "\n",
        "\n",
        "\n",
        "# Mode: The value that appears most frequently in a dataset. A dataset can have one mode (unimodal), two modes (bimodal), or more.\n",
        "\n",
        "# When to use:  Useful for categorical data or when looking for the most common value in a dataset.  Not very informative for continuous data as each value may occur only once.\n",
        "\n",
        "# Example: Finding the most common color of car sold.\n",
        "\n",
        "\n",
        "# Example Calculation\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_mode(data):\n",
        "  count = Counter(data)\n",
        "  max_count = max(count.values())\n",
        "  mode = [k for k, v in count.items() if v == max_count]\n",
        "  return mode\n",
        "\n",
        "\n",
        "data = [10, 20, 30, 20, 20, 40, 50]\n",
        "mode = calculate_mode(data)\n",
        "print(f\"Mode of data is {mode}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ0ceKM4DGNh",
        "outputId": "3937a35d-cc55-4f43-8442-a3b3237eede6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of data is 30.0\n",
            "Median of the data is 35.0\n",
            "Mode of data is [20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explain the concept of dispersion. How do variance and standard deviation measure the spread of data?"
      ],
      "metadata": {
        "id": "9Kk3cT_5DfHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dispersion AKA variability or spread in statistics describes how spread out the data points in a dataset are.  It measures how much the data values deviate from the central tendency (mean, median, or mode). A high dispersion indicates that the data points are widely scattered, while low dispersion suggests they are clustered closely around the central value.\n",
        "\n",
        "# Variance and standard deviation are the two most common measures of dispersion.\n",
        "\n",
        "# Variance:  The average of the squared differences from the mean.  It gives a sense of how much individual data points deviate from the mean value.  Squaring the differences ensures that both positive and negative deviations contribute positively to the total spread.  However, variance is expressed in squared units, which may not be directly interpretable in the context of the original data.\n",
        "\n",
        "# Example Calculation\n",
        "import math\n",
        "def calculate_variance(data):\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return 0  # Handle empty list\n",
        "    mean = sum(data) / n\n",
        "    variance = sum([(x - mean)**2 for x in data]) / n\n",
        "    return variance\n",
        "\n",
        "data = [10, 20, 30, 40, 50]\n",
        "variance = calculate_variance(data)\n",
        "print(f\"Variance of data is {variance}\")\n",
        "\n",
        "\n",
        "# Standard Deviation: The square root of the variance. It is expressed in the same units as the original data, making it easier to interpret in the context of the problem. The standard deviation provides a measure of the typical distance between data points and the mean.  A larger standard deviation means more spread-out data.\n",
        "\n",
        "# Example Calculation\n",
        "def calculate_standard_deviation(data):\n",
        "    variance = calculate_variance(data)\n",
        "    std_dev = math.sqrt(variance)\n",
        "    return std_dev\n",
        "\n",
        "data = [10, 20, 30, 40, 50]\n",
        "std_dev = calculate_standard_deviation(data)\n",
        "print(f\"Standard deviation of data is {std_dev}\")\n",
        "\n",
        "# In summary, variance and standard deviation both quantify the spread of data around the mean. Standard deviation is generally preferred as it shares the same unit as the data, facilitating easier interpretation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjepoVefDgQz",
        "outputId": "cd312129-ef8f-4df9-f688-092a1e75da15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of data is 200.0\n",
            "Standard deviation of data is 14.142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is a box plot, and what can it tell you about the distribution of data?"
      ],
      "metadata": {
        "id": "iM25ExShEa4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A box plot (or box-and-whisker plot) is a graphical representation of the distribution of numerical data. It visually displays the five-number summary of a dataset:\n",
        "\n",
        "# 1. Minimum: The smallest value in the dataset (excluding outliers).\n",
        "# 2. First Quartile (Q1 or 25th percentile): The value below which 25% of the data falls.\n",
        "# 3. Median (Q2 or 50th percentile): The middle value of the dataset.\n",
        "# 4. Third Quartile (Q3 or 75th percentile): The value below which 75% of the data falls.\n",
        "# 5. Maximum: The largest value in the dataset (excluding outliers).\n",
        "\n",
        "# The box in the box plot represents the interquartile range (IQR), which is the difference between the third and first quartiles (IQR = Q3 - Q1).  It contains the middle 50% of the data. A line inside the box indicates the median.  The \"whiskers\" extend from the box to the minimum and maximum values, but typically, they do not extend beyond 1.5 times the IQR from the quartiles. Data points beyond the whiskers are considered outliers and are often plotted as individual points.\n",
        "\n",
        "# What a box plot tells you about the distribution of data:\n",
        "\n",
        "# 1. Central Tendency: The median line within the box gives an idea of the central value of the data.\n",
        "# 2. Spread/Dispersion: The length of the box (IQR) shows the spread of the middle 50% of the data. Longer boxes indicate more variability, while shorter boxes indicate less variability. The whiskers further illustrate the overall range of the data.\n",
        "# 3. Skewness: The position of the median within the box can indicate skewness. If the median is closer to Q1, the data is right-skewed (positive skew), with a longer right tail. If the median is closer to Q3, the data is left-skewed (negative skew), with a longer left tail.  A perfectly symmetrical distribution will have the median in the center of the box.\n",
        "# 4. Outliers:  Outliers are clearly visible as individual points plotted beyond the whiskers.  They can indicate unusual observations or errors in the data.  They can significantly affect the mean but have less impact on the median.\n",
        "# 5. Comparisons: Box plots are excellent for comparing the distribution of multiple datasets side by side. You can quickly visually compare their central tendencies, spreads, and shapes.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# Imagine two datasets representing the test scores of two different classes.\n",
        "# A box plot can reveal whether one class has higher scores on average (comparing medians),\n",
        "# whether scores are more spread out in one class (comparing IQRs), or if either class has unusually high or low scores (outliers)."
      ],
      "metadata": {
        "id": "0SSV5kIXEdpN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Discuss the role of random sampling in making inferences about populations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vfJnSNs3FIEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Random sampling is crucial for making accurate inferences about populations because it helps to ensure that the sample is representative of the population from which it's drawn.  If a sample is not representative, conclusions drawn from it may not accurately reflect the characteristics of the entire population.\n",
        "\n",
        "# Here's a breakdown of its role:\n",
        "\n",
        "# 1. Reducing Bias: Random sampling minimizes bias by giving every member of the population an equal chance of being selected. Non-random sampling methods, like convenience sampling or voluntary response sampling, can lead to samples that overrepresent certain groups and underrepresent others, introducing systematic errors into the results.\n",
        "\n",
        "# 2. Generalizability:  A randomly selected sample allows researchers to generalize their findings to the larger population with a known degree of confidence.  The larger and more random the sample, the more confident we can be in the generalizability of our results.\n",
        "\n",
        "# 3. Statistical Inference: Random sampling is a fundamental requirement for many statistical methods used to make inferences about populations. Statistical tests rely on the assumption that the sample is a random representation of the population to calculate probabilities and draw conclusions.  For instance, confidence intervals and hypothesis testing assume random sampling.\n",
        "\n",
        "# 4. Estimation of Population Parameters:  Random samples provide a basis for estimating population parameters (like the population mean or proportion) with a certain margin of error.  Random sampling allows us to quantify the uncertainty associated with these estimates.  We can calculate confidence intervals that express a range within which the true population parameter is likely to fall.\n",
        "\n",
        "# 5. Evaluating Treatment Effects (in experiments): In experimental designs, random assignment of participants to treatment and control groups is crucial. This random assignment helps ensure that any observed differences between groups are due to the treatment and not pre-existing differences between the groups.\n",
        "\n",
        "# Example: Suppose you want to estimate the average income of all households in a city.  A random sample of households from across the city would provide a more accurate estimate than simply surveying households in a wealthy neighborhood.  The random sample reduces the likelihood of overrepresenting high-income earners and underrepresenting low-income earners, leading to a more accurate estimate of the city's overall average income.\n",
        "\n",
        "# In summary: Random sampling is a cornerstone of sound statistical practice. It allows researchers to collect data that is representative of the population of interest, reducing bias and enabling accurate inferences about that population."
      ],
      "metadata": {
        "id": "RMfgcW3FPQDy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Explain the concept of skewness and its types. How does skewness affect the interpretation of data?"
      ],
      "metadata": {
        "id": "k7MUDEQ1QeLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.  In simpler terms, it describes the lack of symmetry in a dataset's distribution. A perfectly symmetrical distribution (like a normal distribution) has a skewness of zero.\n",
        "\n",
        "# Types of Skewness:\n",
        "\n",
        "# 1. Positive Skew (Right Skew):  The tail on the right side of the distribution is longer or fatter than the left side.  The mean is typically greater than the median, which is greater than the mode.  This indicates that there are some relatively high values that pull the mean to the right.\n",
        "\n",
        "# Example: Income distribution.  Most people have moderate incomes, but a small number of individuals have very high incomes, creating a long tail to the right.\n",
        "\n",
        "# 2. Negative Skew (Left Skew): The tail on the left side of the distribution is longer or fatter than the right side.  The mean is typically less than the median, which is less than the mode.  This indicates that there are some relatively low values pulling the mean to the left.\n",
        "\n",
        "# Example:  Test scores where most students perform well, but a few students score very low.\n",
        "\n",
        "\n",
        "# 3. Zero Skew:  The distribution is perfectly symmetrical.  The mean, median, and mode are all equal.\n",
        "\n",
        "# Example:  A perfectly normal distribution.\n",
        "\n",
        "\n",
        "# How Skewness Affects Interpretation:\n",
        "\n",
        "# 1. Misleading Central Tendency:  In skewed distributions, the mean can be a misleading measure of central tendency because it is influenced by outliers. The median is often a better representation of the typical value in skewed data.\n",
        "\n",
        "# 2. Impact on Statistical Tests:  Many statistical tests assume normality (zero skew).  If the data is significantly skewed, the results of these tests may be unreliable. Transformations (like taking the logarithm of the data) can sometimes be applied to reduce skewness and make the data more suitable for these tests.\n",
        "\n",
        "\n",
        "# 3. Understanding Data Distribution: Skewness provides insights into the shape of the data distribution.  Understanding the shape is important for selecting appropriate statistical methods and drawing meaningful conclusions. For instance, a long tail on one side suggests the presence of outliers or extreme values.\n",
        "\n",
        "# 4. Business Decisions: In business applications, skewness can significantly affect decision-making. For example, in sales forecasting, a right-skewed distribution of sales might indicate a higher chance of unusually high sales, while a left skew might indicate a risk of lower sales.\n",
        "\n",
        "# Example (Illustrative -  you'd typically visualize this with a histogram):\n",
        "\n",
        "# Imagine a dataset of house prices.  If there are a few extremely expensive mansions, the distribution will be right-skewed.  The mean house price would be higher than the median due to the influence of these expensive outliers.  In this case, the median price would be a more representative measure of the typical house price.  If we used the mean to estimate the \"average\" price, our estimate would be distorted upwards.\n"
      ],
      "metadata": {
        "id": "-q8Y0XwFQwtw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the interquartile range (IQR), and how is it used to detect outliers?"
      ],
      "metadata": {
        "id": "7hvHy2YsReIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 − Q1.\n",
        "\n",
        "# How IQR is used to detect outliers:\n",
        "\n",
        "# 1. Calculate the IQR: Find the difference between the third quartile (Q3) and the first quartile (Q1) of your dataset.\n",
        "\n",
        "# 2. Determine the outlier boundaries:\n",
        "#    - Lower bound: Q1 - 1.5 * IQR\n",
        "#    - Upper bound: Q3 + 1.5 * IQR\n",
        "\n",
        "# 3. Identify outliers: Any data point that falls below the lower bound or above the upper bound is considered a potential outlier.\n",
        "\n",
        "\n",
        "# Example:\n",
        "def calculate_iqr_and_outliers(data):\n",
        "    data.sort()\n",
        "    n = len(data)\n",
        "    if n < 4:  # Not enough data for quartiles\n",
        "        return None, []\n",
        "\n",
        "    q1_index = (n + 1) // 4 -1 # index of the first quartile\n",
        "    q3_index = (3 * (n + 1)) // 4 - 1 # index of the third quartile\n",
        "\n",
        "    q1 = data[q1_index]\n",
        "    q3 = data[q3_index]\n",
        "\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "\n",
        "    return iqr, outliers\n",
        "\n",
        "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 1000]  # Example with an outlier\n",
        "iqr, outliers = calculate_iqr_and_outliers(data)\n",
        "\n",
        "print(f\"IQR is {iqr}\")\n",
        "print(f\"Outliers are: {outliers}\")\n",
        "\n",
        "\n",
        "data2 = [10,12,15,18,20,21,22,25,28] # Example without outlier\n",
        "iqr, outliers = calculate_iqr_and_outliers(data2)\n",
        "print(f\"IQR is {iqr}\")\n",
        "print(f\"Outliers are: {outliers}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5Mq7L0yRrii",
        "outputId": "f7c2bb15-7d66-4279-e5a5-273287c526d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IQR is 60\n",
            "Outliers are: [1000]\n",
            "IQR is 10\n",
            "Outliers are: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Discuss the conditions under which the binomial distribution is used."
      ],
      "metadata": {
        "id": "woFNO4C9SwtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The binomial distribution is a discrete probability distribution that describes the probability of obtaining exactly k successes in n independent Bernoulli trials, where each trial has the same probability of success, p.\n",
        "\n",
        "# Conditions for using the binomial distribution:\n",
        "\n",
        "# 1. Fixed number of trials (n):  The experiment consists of a fixed number of trials.  For example, flipping a coin 10 times or observing 100 light bulbs to see if they are defective.\n",
        "\n",
        "# 2. Independent trials: The outcome of each trial is independent of the others. The result of one coin flip does not affect the outcome of subsequent flips.  This independence is crucial.\n",
        "\n",
        "# 3. Two possible outcomes (success/failure): Each trial can only result in one of two outcomes: success or failure.  Examples: Heads or tails, defective or not defective, pass or fail a test.\n",
        "\n",
        "# 4. Constant probability of success (p): The probability of success (p) is the same for each trial. When flipping a fair coin, the probability of getting heads is always 0.5 on each individual flip.\n",
        "\n",
        "# Examples where the binomial distribution applies:\n",
        "\n",
        "# * Number of heads in 20 coin flips (n=20, p=0.5, assuming a fair coin).\n",
        "# * Number of defective light bulbs in a sample of 50 (n=50, p = probability of a defective bulb).\n",
        "# * Number of successful free throws in 10 attempts by a basketball player (n=10, p = player's free throw percentage).\n",
        "# * Number of correctly answered questions on a 20-question multiple-choice test (n=20, p= probability of correctly answering a question).\n",
        "\n",
        "# When the conditions are not met, other distributions (e.g., Poisson, hypergeometric, or negative binomial) might be more appropriate."
      ],
      "metadata": {
        "id": "265-v4HxS_AQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Explain the properties of the normal distribution and the empirical rule (68-95-99.7 rule)."
      ],
      "metadata": {
        "id": "mnhw1-tLTmYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Properties of the Normal Distribution:\n",
        "\n",
        "# 1. Symmetrical: The normal distribution is perfectly symmetrical around its mean (μ).  This means that the left and right sides of the curve are mirror images of each other.\n",
        "\n",
        "# 2. Bell-shaped: The distribution has a characteristic bell shape.  The highest point of the curve corresponds to the mean, median, and mode (which are all equal in a normal distribution).\n",
        "\n",
        "# 3. Defined by Mean (μ) and Standard Deviation (σ): The normal distribution is completely determined by its mean (μ), which represents the center of the distribution, and its standard deviation (σ), which measures the spread or variability of the data.  A larger standard deviation results in a wider, flatter curve, while a smaller standard deviation leads to a taller, narrower curve.\n",
        "\n",
        "# 4. Asymptotic Tails: The tails of the normal distribution extend infinitely in both directions, approaching but never touching the horizontal axis.  This means that there's a theoretical possibility, however small, of observing extremely high or low values.\n",
        "\n",
        "# 5. Empirical Rule (68-95-99.7 Rule):  This rule describes the proportion of data that falls within a certain number of standard deviations from the mean:\n",
        "\n",
        "#    * Approximately 68% of the data falls within one standard deviation of the mean (μ ± σ).\n",
        "#    * Approximately 95% of the data falls within two standard deviations of the mean (μ ± 2σ).\n",
        "#    * Approximately 99.7% (almost all) of the data falls within three standard deviations of the mean (μ ± 3σ).\n",
        "\n",
        "# Significance of the Empirical Rule:\n",
        "\n",
        "# * Data Interpretation: The empirical rule provides a quick way to understand the distribution of data. It helps us visualize where most of the data points lie and identify potential outliers.\n",
        "# * Outlier Detection: Values that fall outside of three standard deviations from the mean are considered unusual or outliers.\n",
        "# * Probability Estimation:  The empirical rule allows for a rough estimation of probabilities associated with different ranges of values within a normal distribution.\n",
        "# * Real-World Applications: Many natural phenomena and measurement errors follow a normal distribution, making the empirical rule a valuable tool in various fields, including statistics, engineering, finance, and medicine.\n",
        "\n",
        "\n",
        "# Example:\n",
        "\n",
        "# Let's say the height of adult women follows a normal distribution with a mean of 5'4\" and a standard deviation of 2\".  Using the empirical rule:\n",
        "\n",
        "# * About 68% of women are between 5'2\" and 5'6\" tall (5'4\" ± 2\").\n",
        "# * About 95% of women are between 5'0\" and 5'8\" tall (5'4\" ± 2 * 2\").\n",
        "# * About 99.7% of women are between 4'8\" and 6'0\" tall (5'4\" ± 3 * 2\")."
      ],
      "metadata": {
        "id": "y58BjidVT3bM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 10. Provide a real-life example of a Poisson process and calculate the probability for a specific event."
      ],
      "metadata": {
        "id": "xGZNp9HyUhRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def poisson_probability(k, lam):\n",
        "  \"\"\"Calculates the probability of k events in a Poisson process with rate parameter lambda.\"\"\"\n",
        "  return (lam**k * math.exp(-lam)) / math.factorial(k)\n",
        "\n",
        "# Example: Website Traffic\n",
        "# Suppose a website receives an average of 5 visitors per minute.\n",
        "# What is the probability that exactly 3 visitors arrive in a given minute?\n",
        "\n",
        "# Define the rate parameter (lambda): average number of visitors per minute\n",
        "lam = 5\n",
        "\n",
        "# Define the desired number of events (k): exactly 3 visitors\n",
        "k = 3\n",
        "\n",
        "# Calculate the probability\n",
        "probability = poisson_probability(k, lam)\n",
        "print(f\"The probability of exactly 3 visitors arriving in one minute is: {probability:.4f}\")\n",
        "\n",
        "\n",
        "# Another example: Car Accidents\n",
        "# On average 2 accidents occur per day at a particular intersection\n",
        "# what is the probability of observing 4 accidents at the intersection today\n",
        "\n",
        "lam = 2\n",
        "k = 4\n",
        "probability = poisson_probability(k, lam)\n",
        "print(f\"The probability of observing 4 accidents today is {probability:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa6zNPUlUirB",
        "outputId": "ec2d97db-bc5a-46e9-baf7-91cf19cf543b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of exactly 3 visitors arriving in one minute is: 0.1404\n",
            "The probability of observing 4 accidents today is 0.0902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Explain what a random variable is and differentiate between discrete and continuous random variables."
      ],
      "metadata": {
        "id": "5F8_NJwXUsUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A random variable is a variable whose value is a numerical outcome of a random phenomenon.  It's a function that maps the outcomes of a random experiment to numerical values.\n",
        "\n",
        "# Discrete Random Variable:\n",
        "\n",
        "# A discrete random variable can only take on a finite number of values or a countably infinite number of values. These values are often integers, but they don't have to be.  The key is that there are gaps between the possible values.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# * Number of heads when flipping a coin four times (can be 0, 1, 2, 3, or 4).\n",
        "# * Number of cars passing a certain point on a highway in an hour (0, 1, 2, 3,...).\n",
        "# * Number of defective items in a batch of 100 (0, 1, 2,...100).\n",
        "\n",
        "\n",
        "# Continuous Random Variable:\n",
        "\n",
        "# A continuous random variable can take on any value within a given range or interval.  There are no gaps between the possible values.  Typically, continuous random variables represent measurements.\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# * Height of a student.\n",
        "# * Weight of an object.\n",
        "# * Temperature of a room.\n",
        "# * Time taken to complete a task.\n",
        "\n",
        "# Key Differences:\n",
        "\n",
        "# 1. Possible Values:  Discrete variables have distinct, separate values, while continuous variables can take on any value within a range.\n",
        "\n",
        "# 2. Probability:  The probability of a discrete random variable taking on a specific value is a non-zero number.  The probability of a continuous random variable taking on a specific value is zero.  Instead, we talk about the probability that a continuous random variable falls within a certain interval.\n",
        "\n",
        "# 3. Visualization:  The probability distribution of a discrete variable is often represented by a probability mass function (PMF), while the probability distribution of a continuous variable is represented by a probability density function (PDF).\n",
        "\n",
        "# In summary, the difference between discrete and continuous random variables lies in the nature of the possible values and how we describe the probability of those values occurring."
      ],
      "metadata": {
        "id": "axoYZohSUuAs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Provide an example dataset, calculate both covariance and correlation, and interpret the results."
      ],
      "metadata": {
        "id": "4Rp9UYOXVKMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example dataset: Hours studied vs. Exam score\n",
        "hours_studied = np.array([2, 5, 3, 8, 1, 6, 4, 7])\n",
        "exam_score = np.array([60, 80, 70, 90, 50, 85, 75, 92])\n",
        "\n",
        "# Calculate covariance\n",
        "covariance = np.cov(hours_studied, exam_score, ddof=0)[0, 1]  # ddof=0 for population covariance\n",
        "print(f\"Covariance: {covariance}\")\n",
        "\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = np.corrcoef(hours_studied, exam_score)[0, 1]\n",
        "print(f\"Correlation: {correlation}\")\n",
        "\n",
        "# Interpretation:\n",
        "# Covariance: A positive covariance (like the one calculated here) suggests a positive relationship between hours studied and exam score, meaning as one increases, the other tends to increase.  However, the magnitude of the covariance is difficult to interpret on its own because it depends on the scales of the variables.\n",
        "\n",
        "# Correlation:  The correlation coefficient ranges from -1 to +1.  A correlation close to +1 (like we have here) indicates a strong positive linear relationship. This means there's a strong tendency for students who study more hours to get higher exam scores. The closer to 1, the stronger the positive linear relationship."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Ti1DbBVLJc",
        "outputId": "55d6b601-9eb8-4648-8591-9ea9c260ff88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance: 30.625\n",
            "Correlation: 0.9717403276586778\n"
          ]
        }
      ]
    }
  ]
}